{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers accelerate bitsandbytes sentencepiece\n",
        "!pip -q install huggingface_hub"
      ],
      "metadata": {
        "id": "XyF7ENuPnOGi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"google/flan-t5-large\", \n",
        "    device_map=\"auto\", \n",
        "    load_in_8bit=True, \n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z316rkjrRKk",
        "outputId": "d23cdad7-d24e-4ce3-b4d3-53b560cd0f8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-21ebae4wf4f34 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flan_t5(func):\n",
        "    def wrapper(prompt):\n",
        "        to_model = func(prompt)\n",
        "        completion = generator(to_model)\n",
        "        return completion[0]['generated_text']\n",
        "    return wrapper\n",
        "\n",
        "categories = [\n",
        "    \"Science\", \"Technology\", \"Art\", \"Engineering\", \"Mathematics\"\n",
        "]\n",
        "\n",
        "# We're going to generate the few-shot prompt with the T5 model first so we\n",
        "# don't have to do any of that pesky \"prompting\" stuff\n",
        "@flan_t5\n",
        "def create_example(category):\n",
        "    return \"Create a creative example subject that falls under the given category. \" + \\\n",
        "           \"For example 'Transportation': 'buses go rogue', 'Soda Cans': 'mt. dew cans have been exploding!'. \" + \\\n",
        "           f\"Category: {category}\"\n",
        "\n",
        "examples = [create_example(i) for i in categories]\n",
        "examples = list(zip(examples, categories))\n",
        "random.shuffle(examples)\n",
        "examples_prompt = \"\\n\".join([f\"{i}: {j}\" for i,j in examples])\n",
        "\n",
        "@flan_t5\n",
        "def clean_label(text):\n",
        "    return \"Your task is to classify a given label into the following categories: \" + \\\n",
        "           f\"{', '.join(categories)}.\" + \\\n",
        "           f\"For example: {examples_prompt}\" + \\\n",
        "           f\"Label: {text}\""
      ],
      "metadata": {
        "id": "7Yw0D0fRnUZK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_label(\"bridge building\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TAX32j1RnqYe",
        "outputId": "394858d2-3380-42e6-d1e4-82c142ec216e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Engineering'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirty_labels = [\"bridge building\", \"alcoholic\", \"pretzel sculptor\", \"watermelon portrait creator\", \"wifi\"]\n",
        "clean_labels = [clean_label(x) for x in dirty_labels]\n",
        "print(clean_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0gEpbVL1Yya",
        "outputId": "191861c7-19bb-4f72-e584-02c7cc7024e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Engineering', 'Science', 'Science', 'Art', 'Technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I saw another example that used fuzzy scores / probabilities from ChatGPT so here's a way to do that\n",
        "@flan_t5\n",
        "def class_confidence_score(data): # {text: 'text', class: 'class'}\n",
        "    return f\"Classes to choose from are: {', '.join(categories)}. \" + \\\n",
        "            \"Your task is to provide a score from 1-10 for how well a dirty label fits a class name. \" + \\\n",
        "            \"How well does 'addition' fit the 'College Algebra' class? 5 \\n\" + \\\n",
        "            \"How well does 'space travel' fit the 'Space' class? 10 \\n\" + \\\n",
        "            \"How well does 'tomato soup' fit the 'Fruits' class? 6 \\n\" + \\\n",
        "            f\"How well does '{data['text']}' fit the '{data['class']}' class?\"\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def probabilities(x, categories):\n",
        "    scores = {}\n",
        "    for i in categories:\n",
        "        scores[i] = class_confidence_score({'text': x, 'class': i})\n",
        "    softmaxed_scores = softmax(np.array([int(i) for i in scores.values()])).tolist()\n",
        "    return softmaxed_scores\n",
        "\n",
        "probabilities('bridge building', categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5GML29ZqfhA",
        "outputId": "39345cea-f086-406c-f4e6-7d152fd0db2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.017065385160357122,\n",
              " 0.017065385160357122,\n",
              " 0.017065385160357122,\n",
              " 0.9317384593585716,\n",
              " 0.017065385160357122]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjA7fMAy1HY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}