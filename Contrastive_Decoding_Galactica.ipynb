{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Decoding with Galactica and Huggingface Transformers\n",
        "\n",
        "[aicrumb](https://twitter.com/aicrumb)\n",
        "\n",
        "Introducing CSearch from Huggingface Blog: https://huggingface.co/blog/introducing-csearch\n",
        "\n",
        "> Natural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, ___Contrastive Search___, for neural text generation. Contrastive search is originally proposed in _\"A Contrastive Framework for Neural Text Generation\"_ <a href='#references'>[1]</a> ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work,  _\"Contrastive Search Is What You Need For Neural Text Generation\"_ <a href='#references'>[2]</a> ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need)), the authors further demonstrate that contrastive search can generate human-level text using **off-the-shelf** language models across **16** languages."
      ],
      "metadata": {
        "id": "B4ppGSYXiZUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!pip install git+https://github.com/paperswithcode/galai -q\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "fVBttaDbfBYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IP3Rx7v6dKEO",
        "outputId": "2694951a-f045-4f0c-a6f1-5cb5df198ceb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is a popular choice for sequence-to-sequence models. It consists of a stack of encoder and decoder layers, each of which is composed of a multi-head self-attention mechanism and a feed-forward network. The encoder'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@title load model and test normal generation\n",
        "\n",
        "import galai as gal\n",
        "model = gal.load_model(\"base\", num_gpus=1)\n",
        "clear_output()\n",
        "model.generate(\"The Transformer architecture [START_REF]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title contrastive generation\n",
        "\n",
        "from galai.utils import escape_custom_split_sequence\n",
        "import torch\n",
        "\n",
        "input_text = \"The Transformer architecture [START_REF]\"\n",
        "max_new_tokens = 256\n",
        "\n",
        "texts = [escape_custom_split_sequence(input_text)]\n",
        "\n",
        "new_doc = True\n",
        "if new_doc:\n",
        "    pad_id = model.tokenizer.padding[\"pad_id\"]\n",
        "    pad_token = model.tokenizer.id_to_token(pad_id)\n",
        "    texts = [pad_token + t for t in texts]\n",
        "\n",
        "list_encoded = model.tokenizer.encode_batch(texts)\n",
        "context_tokens = [encoded.ids for encoded in list_encoded]\n",
        "input_v = torch.LongTensor(context_tokens).to(model.model.device)\n",
        "\n",
        "out = model.model.generate(\n",
        "    input_v, \n",
        "    max_new_tokens = max_new_tokens, \n",
        "    return_dict_in_generate=True, \n",
        "    output_hidden_states=True, \n",
        "    penalty_alpha=0.6, \n",
        "    top_k=4\n",
        ")\n",
        "\n",
        "output = model.tokenizer.decode_batch(\n",
        "    out['sequences'].tolist(), \n",
        "    skip_special_tokens=False\n",
        ")[0].lstrip('<pad>')\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "sHbgK3x3ele0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38af4c8-e894-4a3b-8e92-c7ab40799567"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is used for the encoder and decoder.\n",
            "\n",
            "We use 12 layers for the encoder and 6 layers for the decoder. The embedding size is 512 and the hidden size is 2048. The dropout rate is 0.1. The Adam optimizer is used with β 1 = 0.9, β 2 = 0.98, and = 10 −9 . The learning rate is 0.0001 for pretraining and 0.00001 for fine-tuning.\n",
            "\n",
            "# 4.2 Results\n",
            "\n",
            "Table 1 shows the results of our model and previous state-of-the-art models. We can see that our model performs competitively with the previous SOTA models, which demonstrates the effectiveness of our model. In addition, we compare the performance of different pretraining methods, as shown in Table 2. It can be seen that BERT-like models are better than RoBERTa-like models, which is consistent with the results of [START_REF] How Multilingual is Multilingual BERT?, Pires[END_REF]. The reason is that BERT can capture long-range dependencies,\n"
          ]
        }
      ]
    }
  ]
}